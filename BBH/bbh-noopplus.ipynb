{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "d8de73a1d2de4b08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup model connection\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from converter.converter import *\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"VLLM_API_KEY\"),\n",
    "    base_url=\"http://134.76.18.30:8085/v1\"\n",
    ")\n",
    "model = \"meta-llama/Llama-3.1-8B-Instruct\""
   ],
   "id": "d9041075d6f9d77e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load original dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d57750e572255fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_subsets(dataset_name: str) -> list:\n",
    "    import requests\n",
    "    headers = {\"Authorization\": f\"Bearer {os.getenv('HF_TOKEN')}\"}\n",
    "    API_URL = f\"https://datasets-server.huggingface.co/splits?dataset={dataset_name}\"\n",
    "    data = requests.get(API_URL, headers=headers).json()\n",
    "    return [subset[\"config\"] for subset in data[\"splits\"]]"
   ],
   "id": "e0d3ac6463cf3c8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load spaCy for POS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "selected_split = \"test\"\n",
    "dataset_name = \"maveriq/bigbenchhard\"\n",
    "modified_dataset_name = \"BBH-NoOp-Plus\"\n",
    "\n",
    "hf_username = \"LFrancis\"\n",
    "repo_id = f\"{hf_username}/{modified_dataset_name}\""
   ],
   "id": "da48ae5c0f3b1307",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subsets = get_subsets(dataset_name)\n",
    "def reset_dict():\n",
    "    data_dict = DatasetDict()\n",
    "    for s in subsets:\n",
    "        d = load_dataset(dataset_name, s)[\"train\"]\n",
    "        df = pd.DataFrame(d)\n",
    "        dataset_dict[s] = Dataset.from_pandas(df)\n",
    "    return data_dict\n",
    "dataset_dict = reset_dict()\n",
    "dataset_dict"
   ],
   "id": "7b4fe193ef4b75e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "## Paraphrase Type: Naive Addition",
   "id": "e2d4da176f5705ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for subset in subsets:\n",
    "    dataset_dict[subset+\"_naive\"] = convert_naive(pd.DataFrame(dataset_dict[subset]), question_column=\"input\")\n",
    "pprint(list(dataset_dict.keys()))"
   ],
   "id": "2db48b93821d5294",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "upload(dataset_dict, repo_id)\n",
    "dataset_dict = reset_dict()"
   ],
   "id": "acc9a7a357a4e62f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paraphrase Type: Addition",
   "id": "6f70d4a9110a3bab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for subset in subsets:\n",
    "    dataset_dict[subset+\"_addition\"] = convert_additional(pd.DataFrame(dataset_dict[subset]), client, model, question_column=\"input\")\n",
    "pprint(list(dataset_dict.keys()))"
   ],
   "id": "153c4cdf0b316da4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "upload(dataset_dict, repo_id)\n",
    "dataset_dict = reset_dict()"
   ],
   "id": "bca5666b17c6e3f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paraphrase Type: Lexicon-Changes\n",
   "id": "3af5f01884e9f923"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for subset in subsets:\n",
    "    dataset_dict[subset+\"_lexicon\"] = convert_lexicon(pd.DataFrame(dataset_dict[subset]), client, model,nlp, question_column=\"input\")\n",
    "pprint(list(dataset_dict.keys()))"
   ],
   "id": "d0b5d00a4ba54e62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "upload(dataset_dict, repo_id)\n",
    "dataset_dict = reset_dict()"
   ],
   "id": "e4844f39cdde23bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paraphrase Type: Syntax-Changes\n",
   "id": "9311a162e96e675"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for subset in subsets:\n",
    "    dataset_dict[subset+\"_syntax\"] = convert_syntax(pd.DataFrame(dataset_dict[subset]),nlp, question_column=\"input\")\n",
    "pprint(list(dataset_dict.keys()))"
   ],
   "id": "5d1018e2aa2bb754",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "upload(dataset_dict, repo_id)\n",
    "dataset_dict = reset_dict()"
   ],
   "id": "4f75e9dd4f309d4a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
