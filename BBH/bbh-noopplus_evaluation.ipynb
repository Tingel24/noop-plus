{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:21:37.488517Z",
     "start_time": "2025-01-06T14:21:33.906285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from dotenv import load_dotenv\n",
    "from typing_extensions import override\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "# Load MMLU Dataset\n",
    "dataset_name = \"LFrancis/MMLU-NoOp-Plus\"\n",
    "baseline_dataset_name = \"cais/mmlu\"\n",
    "subset = \"all_lexicon\"\n",
    "dataset = load_dataset(dataset_name, subset)[\"train\"]\n",
    "\n",
    "# VLLM API Configuration\n",
    "BASE_URL = \"http://134.76.18.30:8085/v1/chat/completions\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer \"+os.getenv(\"VLLM_API_KEY\")}\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "EVALUATED_MODEL_PATH = dataset_name+\"_\"+subset + \"_evaluated_\" + MODEL_NAME\n",
    "BASELINE_MODEL_PATH = baseline_dataset_name + \"_evaluated_\" + MODEL_NAME\n",
    "if not os.path.exists(EVALUATED_MODEL_PATH):\n",
    "    dataset.save_to_disk(EVALUATED_MODEL_PATH)\n",
    "#dataset.save_to_disk(EVALUATED_MODEL_PATH)\n",
    "options = [\"A\", \"B\", \"C\", \"D\"]"
   ],
   "id": "62b73412d8b897a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.53M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "690219a604e44f228a3497a10a73b6e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/14042 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f57a371b87154f86ba2d5135963fd658"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14042 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd752b0fa0bc4fd69970d393ec24219c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:21:41.614826Z",
     "start_time": "2025-01-06T14:21:41.595350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "import requests\n",
    "\n",
    "\n",
    "# Helper Functions\n",
    "def create_chat_messages(question, choices, subject, sys_msg):\n",
    "    \"\"\"\n",
    "    Create a formatted list of chat messages for the chat model.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"{question}\\n\" + \"\\n\".join(\n",
    "        [f\"{opt}. {choice}\" for opt, choice in zip(options, choices)]\n",
    "    ) + \"\\nAnswer:\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sys_msg.format(subject)},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "\n",
    "def query_vllm_api(payload):\n",
    "    \"\"\"\n",
    "    Send a query to the VLLM API and return the response.\n",
    "    \"\"\"\n",
    "    response = requests.post(BASE_URL, json=payload, headers=HEADERS, timeout=120)\n",
    "    response.raise_for_status()  # Raise an error for HTTP issues\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def evaluate_question(entry):\n",
    "    # Step 1: Generate reasoning (CoT) response\n",
    "    sys_msg = \"The following are multiple choice questions (with answers) about {}.\"\n",
    "    question, choices, subject = entry[\"question\"], entry[\"choices\"], entry[\"subject\"]\n",
    "\n",
    "    messages = create_chat_messages(question, choices, subject, sys_msg)\n",
    "\n",
    "    cot_payload = {\n",
    "        \"model\": MODEL_NAME,  # Specify model\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 200,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    "\n",
    "    cot_response = query_vllm_api(cot_payload)\n",
    "    if \"object\" in cot_response.keys() and cot_response[\"object\"] == \"error\":\n",
    "        raise Exception(cot_response[\"message\"])\n",
    "\n",
    "\n",
    "    cot_text = cot_response[\"choices\"][0][\"message\"][\"content\"].strip()  # Extract CoT reasoning\n",
    "\n",
    "    # Step 2: Calculate logprobs for each choice\n",
    "    final_prompt = f\"{cot_text}\\n\"+ \"\\n\".join(\n",
    "        [f\"{opt}. {choice}\" for opt, choice in zip(options, choices)]\n",
    "    )+ \"Final Answer: \"\n",
    "    logprobs = {}\n",
    "    for idx, option in enumerate(options):\n",
    "        choice_messages = [\n",
    "            *messages,\n",
    "            {\"role\": \"system\", \"content\": sys_msg.format(subject)},\n",
    "            {\"role\": \"user\", \"content\": final_prompt + f\" {option}\"}\n",
    "        ]\n",
    "        choice_payload = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": choice_messages,\n",
    "            \"max_tokens\": 1,\n",
    "            \"temperature\": 0.0,\n",
    "            \"prompt_logprobs\": 0\n",
    "        }\n",
    "        choice_response = query_vllm_api(choice_payload)\n",
    "        if \"prompt_logprobs\" not in choice_response:\n",
    "            raise Exception(f\"No prompt logprobs found for {option}\")\n",
    "        logprobs[option] = list(choice_response[\"prompt_logprobs\"][-6].values())[0][\"logprob\"]\n",
    "    entry[\"logprobs\"] = logprobs\n",
    "    return entry\n",
    "\n",
    "def is_correct(entry):\n",
    "    \"\"\"\n",
    "    Determines if the choice with the lowest log probability corresponds to the correct answer.\n",
    "\n",
    "    Args:\n",
    "        entry (dict): A dictionary containing the question, choices, answer index, and logprobs.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the option with the lowest logprob matches the correct answer index, False otherwise.\n",
    "    \"\"\"\n",
    "    # Extract logprobs and the correct answer index\n",
    "    logprobs = entry['logprobs']\n",
    "    if logprobs == {} or logprobs == {'A': None, 'B': None, 'C': None, 'D': None}:\n",
    "        return False\n",
    "    correct_answer_index = entry['answer']\n",
    "\n",
    "    # Find the key (A, B, C, D) with the lowest logprob\n",
    "    highest_logprob_option = max(logprobs, key=logprobs.get)\n",
    "\n",
    "    # Map the key to its corresponding index (0 for 'A', 1 for 'B', etc.)\n",
    "    options = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    highest_logprob_index = options.index(highest_logprob_option)\n",
    "\n",
    "    # Check if the lowest logprob index matches the correct answer index\n",
    "    return highest_logprob_index == correct_answer_index\n",
    "\n",
    "\n",
    "def process_dataset(dataset: Dataset, numproc=1):\n",
    "    \"\"\"\n",
    "    Process the dataset using Dataset.map.\n",
    "    \"\"\"\n",
    "\n",
    "    def process_entry(entry):\n",
    "        if \"logprobs\" in entry.keys() and (entry[\"logprobs\"] != {} or entry[\"logprobs\"] != {'A': None, 'B': None, 'C': None, 'D': None}) :\n",
    "            return entry\n",
    "        try:\n",
    "            return evaluate_question(entry)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing entry: {entry}, Exception: {e}\")\n",
    "            entry[\"logprobs\"] = {}\n",
    "            return entry\n",
    "    return dataset.map(process_entry, with_indices=False, num_proc=numproc)"
   ],
   "id": "c60fc0802c475748",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:21:42.797942Z",
     "start_time": "2025-01-06T14:21:42.793522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_dataset(dataset):\n",
    "    # Save the updated dataset to a temporary location\n",
    "    temp_path = \"temp\"\n",
    "    dataset.save_to_disk(temp_path)\n",
    "\n",
    "    # Overwrite the original dataset directory\n",
    "    import shutil\n",
    "    original_path = EVALUATED_MODEL_PATH\n",
    "\n",
    "    # Remove the old dataset and replace it with the new one\n",
    "    shutil.rmtree(original_path)  # Remove the old dataset directory\n",
    "    shutil.move(temp_path, original_path)"
   ],
   "id": "e0ee2b6c396f28dd",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:53:26.575657Z",
     "start_time": "2025-01-06T14:53:25.884238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(is_continue = False, numproc=1):\n",
    "    \"\"\"\n",
    "    Main function to evaluate the dataset asynchronously.\n",
    "    \"\"\"\n",
    "    if is_continue:\n",
    "        selected_dataset = load_from_disk(EVALUATED_MODEL_PATH)\n",
    "    else:\n",
    "        selected_dataset = dataset\n",
    "    # Process the dataset asynchronously\n",
    "    processed_dataset = process_dataset(selected_dataset, numproc)\n",
    "    print(processed_dataset)\n",
    "\n",
    "    # Save the updated dataset\n",
    "    update_dataset(processed_dataset)\n",
    "# Run the script\n",
    "main(True, 1)"
   ],
   "id": "6c05b4e127512852",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/14042 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6606c7a1884c411091b6385e5fc452c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'subject', 'choices', 'answer', 'logprobs'],\n",
      "    num_rows: 14042\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14042 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41f4ba854e454aa3bc3e280dabb6a686"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T15:50:06.137375Z",
     "start_time": "2025-01-06T15:50:05.721856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save or process results as needed\n",
    "selected_dataset = load_from_disk(EVALUATED_MODEL_PATH)\n",
    "score = [is_correct(result) for result in selected_dataset]\n",
    "score = sum(score) / len(score)\n",
    "print(\"Accuracy\", score)"
   ],
   "id": "d6511a8a3f86cbc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.6826662868537245\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T15:50:07.389897Z",
     "start_time": "2025-01-06T15:50:07.021903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "baseline_dataset = load_from_disk(BASELINE_MODEL_PATH)\n",
    "score = [is_correct(result) for result in baseline_dataset]\n",
    "score = sum(score) / len(score)\n",
    "print(\"Baseline Accuracy\", score)"
   ],
   "id": "57d06cdc2ef4c6ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy 0.7026776812419884\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ee329def26a8c925"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
