{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:18:22.540597Z",
     "start_time": "2025-01-27T14:18:22.530530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "\n",
    "def preprocess(dataset_name, subset):\n",
    "    dataset = load_dataset(dataset_name, subset)[subset]\n",
    "    dataset = dataset.add_column(\"subset\", [subset] * len(dataset))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_dataset_from_subsets(split, dataset_name):\n",
    "    # split is noop subset, like lexicon, syntax etc\n",
    "    datasets = [preprocess(dataset_name, subset + ((\"_\" + split) if not split == \"\" else \"\")) for subset in\n",
    "                [\"murder_mysteries\", \"team_allocation\", \"object_placements\"]]\n",
    "\n",
    "    return concatenate_datasets(datasets)"
   ],
   "id": "e310bd65ab089fce",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:19:30.398617Z",
     "start_time": "2025-01-27T14:19:24.689466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "# Load GSM8k Dataset\n",
    "dataset_name = \"LFrancis/MuSR-NoOp-Plus\"\n",
    "baseline_dataset_name = \"TAUR-Lab/MuSR\"\n",
    "subset = \"\"\n",
    "dataset = create_dataset_from_subsets(subset, dataset_name)\n",
    "\n",
    "# VLLM API Configuration\n",
    "BASE_URL = \"http://134.76.18.30:8080/v1/chat/completions\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer \" + os.getenv(\"VLLM_API_KEY\")}\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "EVALUATED_MODEL_PATH = dataset_name + \"_\" + subset + \"_evaluated_\" + MODEL_NAME\n",
    "BASELINE_MODEL_PATH = baseline_dataset_name + \"_evaluated_\" + MODEL_NAME\n",
    "if not os.path.exists(EVALUATED_MODEL_PATH):\n",
    "    dataset.save_to_disk(EVALUATED_MODEL_PATH)\n",
    "if not os.path.exists(BASELINE_MODEL_PATH):\n",
    "    def p(dataset_name, subset):\n",
    "        dataset = load_dataset(dataset_name)[subset]\n",
    "        dataset = dataset.add_column(\"subset\", [subset] * len(dataset))\n",
    "        return dataset\n",
    "\n",
    "    datasets = [p(baseline_dataset_name, subset) for subset in\n",
    "                    [\"murder_mysteries\", \"team_allocation\", \"object_placements\"]]\n",
    "\n",
    "    baseline_dataset = concatenate_datasets(datasets)\n",
    "    baseline_dataset.save_to_disk(BASELINE_MODEL_PATH)"
   ],
   "id": "62b73412d8b897a7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:19:30.444551Z",
     "start_time": "2025-01-27T14:19:30.431817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "import requests\n",
    "\n",
    "\n",
    "# Helper Functions\n",
    "def create_chat_messages(narrative, question, sys_msg):\n",
    "    \"\"\"\n",
    "    Create a formatted list of chat messages for the chat model.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"Narrative: {narrative}\\nQ: {question}\\nA: Let's think step by step.\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sys_msg},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "\n",
    "def query_vllm_api(payload):\n",
    "    \"\"\"\n",
    "    Send a query to the VLLM API and return the response.\n",
    "    \"\"\"\n",
    "    response = requests.post(BASE_URL, json=payload, headers=HEADERS, timeout=120)\n",
    "    response.raise_for_status()  # Raise an error for HTTP issues\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def evaluate_question(entry):\n",
    "    # Step 1: Generate reasoning (CoT) response\n",
    "    sys_msg = \"\"\n",
    "    question = entry[\"question\"]\n",
    "    narrative = entry[\"narrative\"]\n",
    "    messages = create_chat_messages(narrative, question, sys_msg)\n",
    "\n",
    "    cot_payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 200,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    "\n",
    "    cot_response = query_vllm_api(cot_payload)\n",
    "    if \"object\" in cot_response.keys() and cot_response[\"object\"] == \"error\":\n",
    "        raise Exception(cot_response[\"message\"])\n",
    "\n",
    "    cot_text = cot_response[\"choices\"][0][\"message\"][\"content\"].strip()  # Extract CoT reasoning\n",
    "\n",
    "    # Step 2: Calculate logprobs for each choice\n",
    "    final_prompt = f\"Only output your answer, no other explanation or addition. the answer will be copy/pasted as is. Only output one of the following choices as the final Answer:\\nQ: {question}\\nChoices: {entry['choices']}\\nFinal Answer: \"\n",
    "    choice_messages = [\n",
    "        *messages,\n",
    "        {\"role\": \"system\", \"content\": cot_text},\n",
    "        {\"role\": \"user\", \"content\": final_prompt}\n",
    "    ]\n",
    "    final_payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": choice_messages,\n",
    "        \"max_tokens\": 10,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stop\": [\"\\n\"],\n",
    "    }\n",
    "    final_response = query_vllm_api(final_payload)\n",
    "    gen_answer = final_response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    entry[\"generated_answer\"] = gen_answer\n",
    "    entry[\"generated_cot\"] = cot_text\n",
    "    return entry\n",
    "\n",
    "\n",
    "def is_correct(entry):\n",
    "    \"\"\"\n",
    "    Determines if the choice with the lowest log probability corresponds to the correct answer.\n",
    "\n",
    "    Args:\n",
    "        entry (dict): A dictionary containing the question, choices, answer index, and logprobs.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the option with the lowest logprob matches the correct answer index, False otherwise.\n",
    "    \"\"\"\n",
    "    # Extract logprobs and the correct answer index\n",
    "    gen_answer: str = entry['generated_answer']\n",
    "    if gen_answer is None:\n",
    "        return False\n",
    "    answer: str = entry['answer_choice']\n",
    "\n",
    "    return gen_answer.find(answer) != -1\n",
    "\n",
    "\n",
    "def process_dataset(dataset: Dataset, numproc=1):\n",
    "    \"\"\"\n",
    "    Process the dataset using Dataset.map.\n",
    "    \"\"\"\n",
    "\n",
    "    def process_entry(entry):\n",
    "        if \"generated_answer\" in entry.keys() and entry[\"generated_answer\"] is not None:\n",
    "            return entry\n",
    "        try:\n",
    "            return evaluate_question(entry)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing entry: {entry}, Exception: {e}\")\n",
    "            entry[\"generated_answer\"] = None\n",
    "            entry[\"generated_cot\"] = None\n",
    "            return entry\n",
    "\n",
    "    return dataset.map(process_entry, with_indices=False, num_proc=numproc)"
   ],
   "id": "c60fc0802c475748",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:19:30.461394Z",
     "start_time": "2025-01-27T14:19:30.458464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_dataset(dataset, is_baseline=False):\n",
    "    # Save the updated dataset to a temporary location\n",
    "    temp_path = \"temp\"\n",
    "    dataset.save_to_disk(temp_path)\n",
    "\n",
    "    # Overwrite the original dataset directory\n",
    "    import shutil\n",
    "    original_path = EVALUATED_MODEL_PATH if is_baseline == False else BASELINE_MODEL_PATH\n",
    "\n",
    "    # Remove the old dataset and replace it with the new one\n",
    "    shutil.rmtree(original_path)  # Remove the old dataset directory\n",
    "    shutil.move(temp_path, original_path)"
   ],
   "id": "e0ee2b6c396f28dd",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:24:39.519826Z",
     "start_time": "2025-01-27T14:23:45.278404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(is_continue=False, is_baseline=False, numproc=1):\n",
    "    \"\"\"\n",
    "    Main function to evaluate the dataset asynchronously.\n",
    "    \"\"\"\n",
    "    if is_baseline:\n",
    "        selected_dataset = load_from_disk(BASELINE_MODEL_PATH)\n",
    "    elif is_continue:\n",
    "        selected_dataset = load_from_disk(EVALUATED_MODEL_PATH)\n",
    "    else:\n",
    "        selected_dataset = dataset\n",
    "    # Process the dataset asynchronously\n",
    "    processed_dataset = process_dataset(selected_dataset, numproc)\n",
    "\n",
    "    # Save the updated dataset\n",
    "    update_dataset(processed_dataset, is_baseline)\n",
    "\n",
    "\n",
    "# Run the script\n",
    "main(True, False, 1)"
   ],
   "id": "6c05b4e127512852",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/756 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "098feba8fa314816962799286576e933"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/756 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e56160482704f009b60e23dfb243c9d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:26:10.229663Z",
     "start_time": "2025-01-27T14:26:09.950687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from converter.converter import save_value_to_json\n",
    "\n",
    "for subset in [\"addition\", \"lexicon\", \"syntax\", \"\", \"naive\", \"typo\", \"scramble\"]:\n",
    "    if subset == \"\":\n",
    "        EVALUATED_MODEL_PATH = dataset_name + \"__evaluated_\" + MODEL_NAME\n",
    "        subset = \"our_baseline\"\n",
    "    else:\n",
    "        EVALUATED_MODEL_PATH = dataset_name + \"_\" + subset + \"_evaluated_\" + MODEL_NAME\n",
    "    if os.path.exists(EVALUATED_MODEL_PATH):\n",
    "        selected_dataset = load_from_disk(EVALUATED_MODEL_PATH)\n",
    "        print(selected_dataset)\n",
    "        score = [is_correct(result) for result in selected_dataset]\n",
    "        score = sum(score) / len(score)\n",
    "        save_value_to_json(subset, score, MODEL_NAME)\n",
    "        print(subset, \"accuracy\", score)\n",
    "    else:\n",
    "        print(\"skipping\", EVALUATED_MODEL_PATH)"
   ],
   "id": "d6511a8a3f86cbc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['narrative', 'question', 'choices', 'answer_index', 'answer_choice', 'subset', 'generated_answer', 'generated_cot'],\n",
      "    num_rows: 756\n",
      "})\n",
      "addition accuracy 0.3425925925925926\n",
      "Dataset({\n",
      "    features: ['narrative', 'question', 'choices', 'answer_index', 'answer_choice', 'subset', 'generated_answer', 'generated_cot'],\n",
      "    num_rows: 756\n",
      "})\n",
      "lexicon accuracy 0.3333333333333333\n",
      "Dataset({\n",
      "    features: ['narrative', 'question', 'choices', 'answer_index', 'answer_choice', 'subset', 'generated_answer', 'generated_cot'],\n",
      "    num_rows: 756\n",
      "})\n",
      "syntax accuracy 0.2962962962962963\n",
      "Dataset({\n",
      "    features: ['narrative', 'question', 'choices', 'answer_index', 'answer_choice', 'subset', 'generated_answer', 'generated_cot'],\n",
      "    num_rows: 756\n",
      "})\n",
      "our_baseline accuracy 0.3425925925925926\n",
      "Dataset({\n",
      "    features: ['narrative', 'question', 'choices', 'answer_index', 'answer_choice', 'subset', 'generated_answer', 'generated_cot'],\n",
      "    num_rows: 756\n",
      "})\n",
      "naive accuracy 0.36507936507936506\n",
      "Dataset({\n",
      "    features: ['narrative', 'question', 'choices', 'answer_index', 'answer_choice', 'subset', 'generated_answer', 'generated_cot'],\n",
      "    num_rows: 756\n",
      "})\n",
      "typo accuracy 0.27116402116402116\n",
      "Dataset({\n",
      "    features: ['narrative', 'question', 'choices', 'answer_index', 'answer_choice', 'subset', 'generated_answer', 'generated_cot'],\n",
      "    num_rows: 756\n",
      "})\n",
      "scramble accuracy 0.25132275132275134\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T20:20:54.981063Z",
     "start_time": "2025-01-23T20:20:54.974770Z"
    }
   },
   "cell_type": "code",
   "source": "selected_dataset[0]",
   "id": "3fc980b09b9110bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'narrative': 'In an aanledrine incuidng benuge jmpniug stie, Mcak\\'s tihlrl-sieenkg atdurvene came to a goeusmre end by a nnhckuau; now, it\\'s up to Dtticeeve Wotinsn to urneavl the ddelay stecres btweeen Mczkeinae and Ana.\\n\\nWtnsion took a glup of his blcak cfofee, stinarg at the neots swlpaerd aocsrs his desk. A mruder csae at a begnue jmnpiug stie was deitfniley out of the odrirany. Tdaoy\\'s vitcim was a ynoug man named Mack, loud mheoutd and ccoky by all acctouns. \\n\\nMack was bugene jnpumig the day he was kellid. Odldy egnouh, aoccirdng to the rcdores, no one else was duoetncmed at the buenge jpnumig site taht day, making this csae eevn mroe puealicr. The first stop for the day was to vsiit one of Mcak\\'s htsuaomees, a wamon naemd Ana. Tehy were seen liaevng in the smae vchleie from tiher srhead hiuosng cmloepx the mirnong of the muerdr, and it was time for Wtoinsn to dig depeer. \\n\\nAs he pllued itno the sehard hnsuoig dwaeivry, a nonpiedrsct car cmae itno sghit. He lreaned from noibeguhrs taht it was fulterenqy uesd by mitpulle rentdsies, but Ana had a peclaiur isnrteet in it. She wulod iissnt on dnirvig wneehevr wtih a guorp of fiderns, laetr mlutlsuieocy cainenlg the car afetr each use. An iscdiynosray of hres maybe, but a part of the pzzlue netnoslhees.\\n\\nWstonin kncoked on the door, Ana oepend it wlariy, tnddwilig a ceilnang cltoh and srapy in her hdnas and geetred him wtih a nourevs nod. Ana gtes noervus and fiegdts with the clenaer and cotlh when qonuesietd. Wntsoin colud sesne pallapbe usaene as he srtaetd aknisg her qtusnioes.\\n\\n\"Ana, did you not join Mack and the otehrs for beunge jumping tadoy?\" Wsotinn qonsteeuid, to wchih she rdpeesond, \"I sinegd up to jump. But I didn\\'t end up gniog touhrgh wtih it.\"\\n\\n\"Any picralatur raeson you didn\\'t join the ohrets, Ana?\" Wsntion prdceeoed. \\n\\nAna took a deep braeth, \"Well sir, my faith deosn\\'t rellay pimret benuge jupnimg. Ttruh be tlod, I was pdearseud soglrtny by Mack. I had even seignd up out of peer peurrsse but couldn\\'t push mylesf.\"\\n\\nIt was true – Mcak was ininsstig that eveonrye in the gruop sluohd bnguee jump. Mcak had rrdeeptoly also been vacol aoubt rilduiincg Ana’s fiath, even egnnaouicrg oehrts to jion him in doing so. It was a sianginfcit factor in tehir rnolhiatisep.\\n\\n\"Ana, did you and Mcak lvaee in the smae car for the bguene jpminug event tihs moinnrg?\" Wstnion getnly psuehd fhteurr.\\n\\n\"Yes. Yes, we did. We aylaws caoropl.\" She repeonsdd wlhie ausilxnoy unisg the claeenr and ctloh on her car’s daabrhosd. Her eeys fieerckld nrlvesuoy bcak to Wstinon, epitxceng the next qesiuton.\\n\\nWsinotn took a deep btraeh, sdiatnng up to leave, \"Ahgirlt Ana, taht soluhd cover eetnrhiyvg for now. We\\'ll be in tcuoh.\"\\n\\nAna nurvlseoy neoddd wohtuit loiknog up form her cnaeling, wrningig the colth reatledpey as Wsnotin waekld aawy, left agian with atehonr pecie to the etagnmiic pzuzle of Mcak\\'s mudrer.\\n\\nThe day was genittg oeldr and Wonstin was gtneitg mroe tired, but the csae was fersh, and he wasn\\'t one to back down. He teggud on his coat as he aeopahcrpd the bfaushl teen wnaiitg for him by the plicoe stiaton.\\n\\n\"Miacezkne, it is?\" he aeskd, eennidtxg his hnad.\\n\\n\"Yeah, that\\'s rhigt.\" The siglht lsip, oilaevrd with balenkt aexinty, ceoifnmrd waht the sohcol rerptos sstgugeed.\\n\\n\"You were at the stie wehn Mcak... erm... you know,\" Wnsotin\\'s vcioe was mechtaoidl, calm -- almsot rootbic. The supcsoiin on Mineazkce was not udefnonud - the srceiuty cmeraas shwoed him byunig nhuncaku a week bfoere. \\n\\nMnckaeize shtfeid on his feet, linookg away broefe arniwensg, \"Yaeh, I was terhe.\"\\n\\nWsniton pleuld out a slaml noootebk, \"Waht wree you donig there, Mnizecake?”\\n\\n“Buenge jmpiung, like Mcak… Then I left. I didn\\'t... I didn\\'t do aytinhng…” Mizkcanee repelid.\\n\\nIrnenlltay, Wsnotin seihgd at the nveer-ednnig waraftell of tegneae asgnt this csae was tirunng itno. \\n\\n“Miaartl arts, huh?” Woitsnn segued, gnrtieusg to a biurse on Mzicenkae’s kcunlkes. “Nnacuhku patuclairrly, I see? Tnniraig deos incdlue the use of toshe, corcret?” \\n\\nThe change in Miezkance’s doeeanmr mrreroid the bsinetrtes in the lsat motnh’s wetaehr – dark eyes rcplaeed wtih ice-cold ones. “Yeah,” he aitdmetd, sniknhrig sitllghy.\\n\\nMekzcinae aywals took prdie in bnieg the bset at erenhvyitg. So when Mcak got evhrnteiyg he wtnead - the piomrtoon to taem captain, the rpcseet, the aoeittntn - it was a hard pill for Mzaekcine to swolalw. Wstinon rememrebed the taem talk, Mezkciane was idened the top catiaddne but it had gone to Mcak iantesd. \\n\\nWaht chilencd it was Mncaekzie’s ramerks about Mack, ehciong werphsis of dtusipe and bnkiecirg, lost in the cerwodd lhcouornm. Trhee wree also mitullpe wtseins roterps of the two seen ainurgg at the buegne jnumipg stie povsuliery. Meknazice had iedend siad dsraniipgag, asomlt enmoiatol thngis auobt Mack – all smenmitg from a petnot berw of jaesuloy, Wiostnn ifeernrd.\\n\\nShinitfg leatr trhguoh the dtrtiues of Miackneze\\'s life, Winotsn deroecvsid the nnckhuau that mhacetd the frcenoiss reorpt. Tehy wree tkecud aawy, but the laeyr of dsut sgguetesd they wreen\\'t a farvoed psoioesssn aryonme. It wasn’t hdiden, it was miceplasd – dsedriacd in the thoers of dnlioepevg muiattry.\\n\\nAs the sun sraettd to set, Wosntin could see wsteisnes, scarteetd aosrcs the prak, retpaeldey pintinog to the bnegue jmpiung slfdioafcng. It oceurcrd to him, then, the nratavrie of the psat days. Minazkcee, jaluoes and wnoergd, oevr and over, at the same shigt. It was qitue a sghit. \\n\\nWnostin, sfnuihflg back to the sattoin, was left wtih one touhhgt - Looks like Mkeinazce had qutie an eufnetvl week.',\n",
       " 'question': 'Who is the most likely murderer?',\n",
       " 'choices': \"['Mackenzie', 'Ana']\",\n",
       " 'answer_index': 0,\n",
       " 'answer_choice': 'Mackenzie',\n",
       " 'subset': 'murder_mysteries_scramble',\n",
       " 'generated_answer': 'Mackenzie',\n",
       " 'generated_cot': \"To determine the most likely murderer, let's analyze the information provided:\\n\\n1. **Mack's death**: Mack was a loud-mouthed and cocky person, which might have made him a target for someone. The fact that no one else was present at the bungee jumping site that day makes it a more isolated incident.\\n\\n2. **Ana's alibi**: Ana was seen leaving with Mack in the same car, but she claims she didn't join the others for bungee jumping due to her faith and personal reasons. She also mentions that Mack was insistent on everyone participating in bungee jumping, which might have created tension.\\n\\n3. **Mizkcanee's alibi**: Mizkcanee was at the bungee jumping site, but he claims he left after bungee jumping and didn't do anything else. However, he seems nervous and agitated during the conversation with Winston.\\n\\n4. **Mizkcanee's past**: Mizk\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T20:20:55.174261Z",
     "start_time": "2025-01-23T20:20:55.130203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_dataset = load_from_disk(BASELINE_MODEL_PATH)\n",
    "score = [is_correct(result) for result in baseline_dataset]\n",
    "score = sum(score) / len(score)\n",
    "save_value_to_json(\"baseline\", score, MODEL_NAME)\n",
    "print(\"Baseline Accuracy\", score)"
   ],
   "id": "57d06cdc2ef4c6ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy 0.3505291005291005\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T20:20:55.224461Z",
     "start_time": "2025-01-23T20:20:55.223073Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "425570df70200025",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
